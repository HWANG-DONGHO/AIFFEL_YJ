{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "afraid-richmond",
   "metadata": {},
   "source": [
    "# 작사가 제일 쉬웠어요!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stuck-fraud",
   "metadata": {},
   "source": [
    "NLP를 통해 멋진 가사를 만들어내는 작사가 모델을 만들어보자."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "accessory-transcript",
   "metadata": {},
   "source": [
    "## 1. 필요한 모듈 import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "peaceful-block",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import glob\n",
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "helpful-resistance",
   "metadata": {},
   "source": [
    "## 2. 데이터 로드 및 개수 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "useful-parks",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "데이터 크기: 187088\n",
      "Examples:\n",
      " ['At first I was afraid', 'I was petrified', 'I kept thinking I could never live without you']\n"
     ]
    }
   ],
   "source": [
    "# 문장의 총 개수 구하기\n",
    "\n",
    "txt_file_path = os.getenv('HOME')+'/aiffel/lyricist/data/lyrics/*'\n",
    "\n",
    "txt_list = glob.glob(txt_file_path)\n",
    "\n",
    "raw_corpus = [] # 문장들이 담길 list\n",
    "\n",
    "# 여러개의 txt 파일을 모두 읽어서 raw_corpus 에 담습니다.\n",
    "for txt_file in txt_list:\n",
    "    with open(txt_file, \"r\") as f:\n",
    "        raw = f.read().splitlines()\n",
    "        raw_corpus.extend(raw)\n",
    "\n",
    "print(\"데이터 크기:\", len(raw_corpus))\n",
    "print(\"Examples:\\n\", raw_corpus[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "capable-legislation",
   "metadata": {},
   "source": [
    "## 3. 데이터 전처리\n",
    "\n",
    "문장을 학습시키기 쉽게 불필요한 부분은 제거하고, 하나의 기준으로 문장을 정리한다.\n",
    "1. 소문자로 변환한 후 양쪽 공백을 지운다.\n",
    "2. 문장에 포함된 특수문자 양쪽에 공백 추가(split하기 위해)\n",
    "3. 문장 내의 2개 이상인 공백을 1개의 공백으로 변환한다.\n",
    "4. a-zA-Z?.!,¿가 아닌 모든 문자를 하나의 공백으로 바꾼다.\n",
    "5. 양쪽 공백을 지운다.\n",
    "6. 모든 전처리가 끝난 문장 앞뒤로 \\<start\\>와 \\<end\\>를 붙인다. (토큰화)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "enormous-network",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<start> at first i was afraid <end>\n"
     ]
    }
   ],
   "source": [
    "def preprocess_sentence(sentence):\n",
    "    sentence = sentence.lower().strip() # 1\n",
    "    sentence = re.sub(r\"([?.!,¿])\", r\" \\1 \", sentence) # 2\n",
    "    # 문자열 앞에 r이 붙으면 해당 문자열이 구성된 그대로 문자열 반환 예를 들어 \\n 같은 경우에도 raw string으로 그대로 출력\n",
    "    # \\1은 재참조 메타 문자.\n",
    "    \n",
    "    sentence = re.sub(r'[\" \"]+', \" \", sentence) # 3\n",
    "    sentence = re.sub(r\"[^a-zA-Z?.!,¿]+\", \" \", sentence) # 4\n",
    "    sentence = sentence.strip() # 5\n",
    "    sentence = '<start> ' + sentence + ' <end>' # 6\n",
    "    return sentence\n",
    "\n",
    "# 이 문장이 어떻게 필터링되는지 확인해 보세요.\n",
    "print(preprocess_sentence(raw_corpus[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "surprising-fourth",
   "metadata": {},
   "source": [
    "* 길이가 0인 문장을 제거한다.\n",
    "* 지나치게 긴 문장은 다른 데이터들이 과도한 Padding을 갖게 하므로 제거한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fundamental-thesaurus",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "156227\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['<start> at first i was afraid <end>',\n",
       " '<start> i was petrified <end>',\n",
       " '<start> i kept thinking i could never live without you <end>',\n",
       " '<start> by my side but then i spent so many nights <end>',\n",
       " '<start> just thinking how you ve done me wrong <end>',\n",
       " '<start> i grew strong <end>',\n",
       " '<start> i learned how to get along and so you re back <end>',\n",
       " '<start> from outer space <end>',\n",
       " '<start> i just walked in to find you <end>',\n",
       " '<start> i would have made you leave your key <end>',\n",
       " '<start> if i had known for just one second <end>',\n",
       " '<start> you d be back to bother me well now go , <end>',\n",
       " '<start> walk out the door <end>',\n",
       " '<start> just turn around <end>',\n",
       " '<start> now , you re not welcome anymore weren t you the one <end>',\n",
       " '<start> who tried to break me with desire ? <end>',\n",
       " '<start> did you think i d crumble ? <end>',\n",
       " '<start> i will survive <end>',\n",
       " '<start> yeah <end>',\n",
       " '<start> i ve got all my life to live <end>',\n",
       " '<start> i ve got all my love to give <end>',\n",
       " '<start> i will survive , i will survive <end>',\n",
       " '<start> yeah , yeah <end>',\n",
       " '<start> it took all the strength i had <end>',\n",
       " '<start> just not to fall apart i m trying hard to mend the pieces <end>',\n",
       " '<start> of my broken heart <end>',\n",
       " '<start> and i spent oh so many nights <end>',\n",
       " '<start> just feeling sorry for myself i used to cry , <end>',\n",
       " '<start> but now i hold my head up high <end>',\n",
       " '<start> and you see me <end>']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 여기에 정제된 문장을 모을 리스트 생성\n",
    "corpus = []\n",
    "\n",
    "for sentence in raw_corpus:\n",
    "    # 우리가 원하지 않는 문장은 건너뛴다.\n",
    "    # 길이가 0이거나 단어의 개수가 15개 이상인 문장을 건너뛴다.\n",
    "    # <start>와 <end>를 제외해야 하므로 13개 \n",
    "    if len(sentence) == 0 : continue\n",
    "    \n",
    "    # 정제를 하고 담는다.\n",
    "    preprocessed_sentence = preprocess_sentence(sentence)\n",
    "    if len(preprocessed_sentence.split()) > 15 : continue\n",
    "    \n",
    "    corpus.append(preprocessed_sentence)\n",
    "    \n",
    "len_list = []\n",
    "for s in corpus:\n",
    "    len_list.append(len(s.split()))\n",
    "    \n",
    "# 정제된 결과를 30개 확인\n",
    "print(len(corpus))\n",
    "\n",
    "corpus[:30]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "parental-functionality",
   "metadata": {},
   "source": [
    "* 그래프로 전처리 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "stable-vision",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import collections\n",
    "\n",
    "# collections.Counter로 len_list의 토큰 개수를 개수 별 딕셔너리 형태로 반환한다.\n",
    "len_count = collections.Counter(len_list)\n",
    "\n",
    "se = pd.Series(len_count)\n",
    "se.sort_index(inplace=True)\n",
    "\n",
    "plt.figure(figsize=(20,5))\n",
    "\n",
    "sns.barplot(x=se.index, y=se.values)\n",
    "plt.title('Histogram of Sentences Length', fontsize=15)\n",
    "plt.xlabel('Length of Sentences', fontsize=13)\n",
    "plt.ylabel('Count', fontsize=13)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "piano-junction",
   "metadata": {},
   "source": [
    "## 4. 데이터 토큰화\n",
    "수학적 연산을 위해 문자 데이터를 Tensor로 변환, tf.tokenizer()를 통해 편리하게 변환할 수 있다.\n",
    "\n",
    "* 각 문자에 대응되는 숫자 사전을 만든다. (12000개)\n",
    "* 사전에 포함되지 않은 단어는 ukn으로 대체한다.\n",
    "* 문자를 단어별로 split한 후 각 문자에 대응하는 숫자로 변환한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "lightweight-reflection",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   2   70  247 ...    0    0    0]\n",
      " [   2    4   53 ...    0    0    0]\n",
      " [   2    4 1066 ...    0    0    0]\n",
      " ...\n",
      " [   2    8    4 ...    0    0    0]\n",
      " [   2   44   17 ...    0    0    0]\n",
      " [   2    6  174 ...    0    0    0]] <keras_preprocessing.text.Tokenizer object at 0x7f56c2878050>\n"
     ]
    }
   ],
   "source": [
    "def tokenize(corpus):\n",
    "    # 12000단어를 기억할 수 있는 tokenizer\n",
    "    # preprocessing을 통해 데이터 전처리를 완료했으므로 filter는 필요 없다.\n",
    "    # 12000단어에 포함되지 못한 단어는 '<unk>'로 대체한다.\n",
    "    tokenizer = tf.keras.preprocessing.text.Tokenizer(\n",
    "        num_words=12000, \n",
    "        filters=' ',\n",
    "        oov_token=\"<unk>\"\n",
    "    )\n",
    "    # corpus를 이용해 tokenizer 내부의 단어장을 완성한다.\n",
    "    tokenizer.fit_on_texts(corpus)\n",
    "    # 준비한 tokenizer를 이용해 corpus를 Tensor로 변환한다.\n",
    "    tensor = tokenizer.texts_to_sequences(corpus)   \n",
    "    # 입력 데이터의 시퀀스 길이를 일정하게 맞춰준다.\n",
    "    # 만약 시퀀스가 짧다면 문장 뒤에 패딩을 붙여 길이를 맞춰준다.\n",
    "    tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor, padding='post', maxlen=15)  \n",
    "    \n",
    "    print(tensor,tokenizer)\n",
    "    return tensor, tokenizer\n",
    "\n",
    "tensor, tokenizer = tokenize(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "institutional-brother",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 : <unk>\n",
      "2 : <start>\n",
      "3 : <end>\n",
      "4 : i\n",
      "5 : ,\n",
      "6 : the\n",
      "7 : you\n",
      "8 : and\n",
      "9 : a\n",
      "10 : to\n"
     ]
    }
   ],
   "source": [
    "#단어사전 확인\n",
    "for idx in tokenizer.index_word:\n",
    "    print(idx, \":\", tokenizer.index_word[idx])\n",
    "    \n",
    "    if idx >= 10: break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "radio-festival",
   "metadata": {},
   "source": [
    "## 5. 데이터셋 생성\n",
    "\n",
    "Tensor에서 모델을 학습할 때 필요한 입력값(source)과 출력값(target)을 나눠 정리한다.\n",
    "\n",
    "* 입력값(source) : \\<start\\>를 포함하고 맨 뒤 값인 \\<end\\>를 제외한 문장, 그러나 마지막 토큰은 \\<end\\>가 아닌 padding일 확률이 매우 높다.\n",
    "* 출력값(target) : \\<start\\> 다음 단어부터 \\<end\\> 까지의 문장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "charitable-workstation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  2  70 247   4  53 708   3   0   0   0   0   0   0   0]\n",
      "[ 70 247   4  53 708   3   0   0   0   0   0   0   0   0]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(156227, 14)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#source, target 문장 생성\n",
    "src_input = tensor[:, :-1]\n",
    "tgt_input = tensor[:, 1:]\n",
    "\n",
    "print(src_input[0])\n",
    "print(tgt_input[0])\n",
    "src_input.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "compact-bridal",
   "metadata": {},
   "source": [
    "* Train set과 Validation set 8 : 2로 분리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "unlikely-cookie",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source Train: (124981, 14)\n",
      "Target Train: (124981, 14)\n",
      "Source Validation: (31246, 14)\n",
      "Target Validation: (31246, 14)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "enc_train, enc_val, dec_train, dec_val = train_test_split(src_input, tgt_input, test_size=0.2)\n",
    "\n",
    "print(\"Source Train:\", enc_train.shape)\n",
    "print(\"Target Train:\", dec_train.shape)\n",
    "print(\"Source Validation:\", enc_val.shape)\n",
    "print(\"Target Validation:\", dec_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "disturbed-candy",
   "metadata": {},
   "source": [
    "## 6. 모델 설계\n",
    "#### 일반 LSTM 모델\n",
    "\n",
    "가사 데이터를 통해 학습할 모델 정의\n",
    "\n",
    "* Embedding Layer : Word2Vec 역할을 하는 Layer, 단어를 저차원의 실수형 밀집 벡터(Dense vector)로 만든다. \n",
    "* LSTM(Long Short-Term Memory) Layer : 은닉층의 메모리 셀에 입력 게이트, 망각 게이트, 출력 게이트를 추가해 불필요한 메모리를 지우고, 필요한 메모리를 저장한다.\n",
    "* Dense Layer : Fully-Connected Layer로 출력값을 결정짓는다.\n",
    "\n",
    "**Embedding(number of sample, embedding_size, input_length)**    \n",
    "number of sample : 단어 집합의 크기. 즉, 총 단어의 개수  \n",
    "embedding_size : 임베딩 벡터의 출력 차원. 결과로서 나오는 임베딩 벡터의 크기  \n",
    "input_length : 입력 시퀀스의 길이  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "recognized-qualification",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, None, 256)         3072256   \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (None, None, 1024)        5246976   \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, None, 1024)        8392704   \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, None, 12001)       12301025  \n",
      "=================================================================\n",
      "Total params: 29,012,961\n",
      "Trainable params: 29,012,961\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "vocab_size = tokenizer.num_words + 1  # 단어사전의 단어 개수 + 0:<pad>\n",
    "embedding_size = 256\n",
    "hidden_size = 1024\n",
    "\n",
    "model = tf.keras.Sequential()\n",
    "model.add(tf.keras.layers.Embedding(vocab_size, embedding_size))\n",
    "model.add(tf.keras.layers.LSTM(hidden_size, return_sequences=True))\n",
    "model.add(tf.keras.layers.LSTM(hidden_size, return_sequences=True))\n",
    "model.add(tf.keras.layers.Dense(vocab_size))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "alleged-token",
   "metadata": {},
   "source": [
    "## 7. 모델 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "stunning-colleague",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "3906/3906 [==============================] - 293s 74ms/step - loss: 3.3536\n",
      "Epoch 2/10\n",
      "3906/3906 [==============================] - 293s 75ms/step - loss: 2.6685\n",
      "Epoch 3/10\n",
      "3906/3906 [==============================] - 295s 75ms/step - loss: 2.3586\n",
      "Epoch 4/10\n",
      "3906/3906 [==============================] - 296s 76ms/step - loss: 2.0825\n",
      "Epoch 5/10\n",
      "3906/3906 [==============================] - 296s 76ms/step - loss: 1.8386\n",
      "Epoch 6/10\n",
      "3906/3906 [==============================] - 296s 76ms/step - loss: 1.6409\n",
      "Epoch 7/10\n",
      "3906/3906 [==============================] - 296s 76ms/step - loss: 1.4766\n",
      "Epoch 8/10\n",
      "3906/3906 [==============================] - 296s 76ms/step - loss: 1.3450\n",
      "Epoch 9/10\n",
      "3906/3906 [==============================] - 298s 76ms/step - loss: 1.2406\n",
      "Epoch 10/10\n",
      "3906/3906 [==============================] - 298s 76ms/step - loss: 1.1620\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f56b9fd7cd0>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer = tf.keras.optimizers.Adam()\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True,\n",
    "    reduction='none'\n",
    ")\n",
    "\n",
    "model.compile(loss=loss, optimizer=optimizer)\n",
    "model.fit(enc_train, dec_train, epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "copyrighted-camcorder",
   "metadata": {},
   "source": [
    "## 8. 문장 생성하기\n",
    "위에서 학습한 모델을 통해 문장을 생성한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "narrow-prison",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 문장 생성 함수\n",
    "def generate_text(model, tokenizer, init_sentence=\"<start>\", max_len=20):\n",
    "    # 테스트를 위해서 입력받은 init_sentence도 일단 텐서로 변환한다.\n",
    "    test_input = tokenizer.texts_to_sequences([init_sentence])\n",
    "    test_tensor = tf.convert_to_tensor(test_input, dtype=tf.int64)\n",
    "    end_token = tokenizer.word_index[\"<end>\"]\n",
    "\n",
    "    # 텍스트를 실제로 생성할때는 루프를 돌면서 단어 하나씩 생성해야 한다. \n",
    "    while True:\n",
    "        predict = model(test_tensor)  # 입력받은 문장의 텐서를 입력한다. \n",
    "        predict_word = tf.argmax(tf.nn.softmax(predict, axis=-1), axis=-1)[:, -1]   # 우리 모델이 예측한 마지막 단어가 바로 새롭게 생성한 단어가 된다. \n",
    "\n",
    "        # 우리 모델이 새롭게 예측한 단어를 입력 문장의 뒤에 붙여 준다. \n",
    "        test_tensor = tf.concat([test_tensor, tf.expand_dims(predict_word, axis=0)], axis=-1)\n",
    "\n",
    "        # 우리 모델이 <end>를 예측했거나, max_len에 도달하지 않았다면  while 루프를 또 돌면서 다음 단어를 예측해야 한다..\n",
    "        if predict_word.numpy()[0] == end_token: break\n",
    "        if test_tensor.shape[1] >= max_len: break\n",
    "\n",
    "    generated = \"\"\n",
    "    # 생성된 tensor 안에 있는 word index를 tokenizer.index_word 사전을 통해 실제 단어로 하나씩 변환한다. \n",
    "    for word_index in test_tensor[0].numpy():\n",
    "        generated += tokenizer.index_word[word_index] + \" \"\n",
    "\n",
    "    return generated   # 이것이 최종적으로 모델이 생성한 자연어 문장입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mathematical-beginning",
   "metadata": {},
   "source": [
    "#### 'i love' 입력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "solved-brown",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<start> i love you so , <end> '"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 문장 생성 함수 실행하여 모델에게 작문 시켜보기\n",
    "generate_text(model, tokenizer, init_sentence=\"<start> i love\", max_len=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "internal-request",
   "metadata": {},
   "source": [
    "#### 'when i was' 입력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "caring-receptor",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<start> when i was shot dat nixga like i started to <end> '"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(model, tokenizer, init_sentence=\"<start> when i was\", max_len=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "plain-auckland",
   "metadata": {},
   "source": [
    "#### 'what would you' 입력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "liked-collaboration",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<start> what would you do ? <end> '"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(model, tokenizer, init_sentence=\"<start> what would you\", max_len=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "foreign-perception",
   "metadata": {},
   "source": [
    "#### 'he was a' 입력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "functioning-vienna",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<start> he was a very much <end> '"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(model, tokenizer, init_sentence=\"<start> he was a\", max_len=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sharing-marketing",
   "metadata": {},
   "source": [
    "#### 'happy' 입력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "incoming-contract",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<start> happy birthday , happy birthday , happy birthday woo , shake ! <end> '"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(model, tokenizer, init_sentence=\"<start> happy\", max_len=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "anticipated-motivation",
   "metadata": {},
   "source": [
    "#### 'i am' 입력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "suspended-luxury",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<start> i am a god <end> '"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(model, tokenizer, init_sentence=\"<start> i am\", max_len=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "substantial-prisoner",
   "metadata": {},
   "source": [
    "#### 'i m' 입력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "diagnostic-retail",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<start> i m the one <end> '"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(model, tokenizer, init_sentence=\"<start> i m\", max_len=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "demographic-marks",
   "metadata": {},
   "source": [
    "## 결론 및 고찰\n",
    "* 학습하는데 많은 시간이 소요되었지만 loss 1.16정도로 낮은 loss를 얻을 수 있었다.\n",
    "* 생성한 문장 중 'when i was' 로 시작하는 문장은 의미를 알 수 없는 문장을 생성해냈다. 해당 문장으로 시작하는 데이터의 경우 학습 데이터가 부족하다면 의미를 알 수 없는 문장을 생성해 낼 수도 있다는 것을 알 수 있었다.\n",
    "* 'i am'과 'i m'이 다른 문장을 생성하는 것을 볼 때 축소된 단어는 다른 단어로 인식하고 있으며, 같은 단어로 인식하게 만들 수 있으면 더 좋은 성능을 낼 수 있을 것이라고 생각한다.\n",
    "* 결론적으로 아주 만족스러운 모델이 만들어졌다고는 생각되지 않는다. 하지만 전처리 과정에서 각 단어들의 특성을 조금 더 반영하고, 더 많은 데이터를 확보해 학습한다면 정말 좋은 가사들을 많이 만들어 낼 수 있을 것 같다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fleet-phoenix",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
